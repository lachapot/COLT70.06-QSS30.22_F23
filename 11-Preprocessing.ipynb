{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa56dd34-48a0-479c-bf30-fbe7ecc648d2",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867924af-e3d3-4424-9446-a24726186591",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bccfc06-1147-41cd-936a-ef07d7f30ab8",
   "metadata": {},
   "source": [
    "### What is lemmatization?\n",
    "\n",
    "Lemmatization is a procedure that reduces the inflectional forms of words to a common base or root. \n",
    "\n",
    "English has minimal inflection (e.g. words can be inflected by number: \"cat\" becomes \"cats\" in the plural). Other languages, however, have much more inflection. Words can vary, for example, according to whether the word is definite or indefinite, and also according to number and gender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd74fd1f-2692-4285-ab9e-ca26266ce8e2",
   "metadata": {},
   "source": [
    "### Creating a lemmatized version of your corpus\n",
    "\n",
    "For methods that rely on word counts (e.g. frequency counts, Tf-idf), it's best to use lemmatized text so that a maximum number of words we want counted togther will be counted together. There is evidence that lemmatization is not necessary, maybe even counterproductive for topic modeling.  \n",
    "\n",
    "> \"Stemming has been found to provide little measurable benefits for topic modeling and can sometimes even be harmful (Schofield and Mimno, 2016).\" (Nguyen et al., \"How We Do Things With Words,\" p. 8)\n",
    "\n",
    "\n",
    "It might be good practice to have a lemmatized and unlemmatized version of your corpus so you can experiment with which one produces the most meaningful outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12483a62-951b-4b78-ae25-961a7e3f7476",
   "metadata": {},
   "source": [
    "**Lemmatizing mutiple files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1786a6-741c-4962-9839-3b7ec6f9e9b4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This loops over multiple files in a directory\n",
    "#but it might make the kernel crash if it runs out memory\n",
    "#If the kernel crash you might have to lemmatize single files at a time (cf. below)\n",
    "#or run nlp.max_length = 200000 (or any large value after loading the model)\n",
    "\n",
    "#Lemmatizing using spaCy for English\n",
    "import spacy\n",
    "import glob\n",
    "\n",
    "#Download the language model you're interested in (this is the English pipeline)\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4167b17-f717-4e76-bd08-651dcaec3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load language model\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe8949f-a25d-446d-93d5-5876f8e5aa9a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set filepath\n",
    "filepath = 'soderberg-corpus/'\n",
    "text_files = glob.glob(f'{filepath}/*.txt')\n",
    "\n",
    "#Loop through the files and open as spacy document\n",
    "for file in text_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        print(file)\n",
    "        document = nlp(text)\n",
    "        \n",
    "    #Lemmatize each file and create new file with '-lemmatized.txt' added to name\n",
    "    outname = file.replace('.txt', '-lemmatized.txt')\n",
    "    with open(outname, 'w', encoding='utf8') as out:   \n",
    "        for token in document:\n",
    "            # Get the lemma for each token\n",
    "            out.write(token.lemma_.lower())\n",
    "            # Insert white space between each token\n",
    "            out.write(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad98f29-9664-4bd7-8878-2fc4b8b14780",
   "metadata": {},
   "source": [
    "**Lemmatizing single files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e25860-21b8-4921-904e-3dc4a853f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing single files\n",
    "\n",
    "#Lemmatizing using spaCy for English\n",
    "import spacy\n",
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1393b043-1e12-47e3-a1d6-7542775dda2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the language model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "#Open your text and create spaCy document\n",
    "filepath = 'soderberg-corpus/1897_Drizzle.txt'\n",
    "text = open(filepath, encoding='utf-8').read()\n",
    "document = nlp(text)\n",
    "\n",
    "outname = filepath.replace('.txt', '-lemmatized.txt')\n",
    "with open(outname, 'w', encoding='utf8') as out:   \n",
    "    for token in document:\n",
    "        # Get the lemma for each token\n",
    "        out.write(token.lemma_.lower())\n",
    "        # Insert white space between each token\n",
    "        out.write(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9551f4-e4e9-45fb-9187-524335050ee0",
   "metadata": {},
   "source": [
    "**Checking over the lemmatized forms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d00f3c-f62c-472a-861c-1e5876ab5d7a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This prints the original word in the text, \n",
    "#a dash, then the lemmatized form that was written to the derivative text document\n",
    "#check if there are places where the model consistently makes mistakes\n",
    "#this prints the first 50 tokens - modify the slice next to document for more\n",
    "for token in document[:50]:\n",
    "    print(token.text + ' - ' + token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109a1388-bcc7-4efb-bfa6-e44206ba936b",
   "metadata": {},
   "source": [
    "# Using spaCy to create a tokenized version of Chinese and Korean texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d482f7de-dac4-4bf6-9af4-36888c2b81da",
   "metadata": {},
   "source": [
    "Some languages do not separate words with spaces. One way to tokenize for these language is to artificially insert spaces in the text. This is called segmentation. We can use spaCy to create a *segmented derivative* of the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928f32d0-863b-4434-a7f4-c4bad603fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ea888e-ab08-43ee-a8cd-0737abab550f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Download the language model you're interested in\n",
    "#e.g. for Chinese: python -m spacy download zh_core_web_sm\n",
    "#e.g. for Korean: ko_core_news_sm\n",
    "#Visit: https://spacy.io/usage/models#languages for more\n",
    "!python -m spacy download ko_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e304f-c614-43bc-a722-3d25a6908cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load language model\n",
    "nlp = spacy.load('ko_core_news_sm')\n",
    "\n",
    "#Create spaCy document\n",
    "text = open('korean-corpus.txt', encoding='utf-8').read()\n",
    "document = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e6989b-18ed-4765-bbdc-bcfaa07d80e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a segmented version of the original text file\n",
    "#Loop through each token in the original text, lemmatize and lowercase each token, \n",
    "#and insert a space between the tokens. Then write them out to new file\n",
    "\n",
    "filepath = 'korean-corpus.txt'\n",
    "outname = filepath.replace('.txt', '-segmented.txt')\n",
    "with open(outname, 'w', encoding='utf8') as out:\n",
    "    for token in document:\n",
    "        # Get the lemma for each token\n",
    "        out.write(token.lemma_.lower())\n",
    "        # Insert white space between each token\n",
    "        out.write(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833de98-dff3-44b6-b3f9-56b712c97b51",
   "metadata": {},
   "source": [
    "The code cell below prints the text as a list of individual tokens (words and punctuation), so you can see how successfully it identified word boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c906307e-5d7c-4073-946b-9c61067b18b4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for token in document:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d2df0d-8406-4e8b-8e51-26cc390bfa85",
   "metadata": {},
   "source": [
    "_Acknowledgements_: This notebook is inspired by Melanie Walshâ€™s [_Introduction to Cultural Analytics & Python_](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/Multilingual/Chinese/03-POS-Keywords-Chinese.html#keyword-extraction)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
