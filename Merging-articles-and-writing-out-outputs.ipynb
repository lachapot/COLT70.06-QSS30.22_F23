{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939aa7e2-7963-4fe4-ad18-c3ec83cc8f75",
   "metadata": {},
   "source": [
    "## Merging all articles into one text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26575d6e-32b6-40a2-a174-0c7625f9a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7951a904-8097-4ccc-8b93-4caf0bc6d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to your corpus directory\n",
    "directory_path = 'soderberg-corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ce9ce6-6d9b-483d-a62e-c24effa9b236",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use glob to get all the text files in the directory\n",
    "text_files = glob.glob(f'{directory_path}/*.txt')\n",
    "text_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6b711d-013a-460d-aab8-e5b5ce4ba5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a new file called \"soderberg-corpus.txt for writing out to\n",
    "#and loop through each text file \n",
    "#open it and read and write out the content \n",
    "#(with a newline to separate the contents of different files)\n",
    "with open(\"soderberg-corpus.txt\", \"w\", encoding='utf-8') as output_file:\n",
    "    for filepath in text_files:\n",
    "        with open(filepath, 'r', encoding='utf-8') as input_file:\n",
    "            text = input_file.read()\n",
    "            output_file.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3efdbc-b049-4b50-b5b4-7b2932eb996a",
   "metadata": {},
   "source": [
    "## Write out Keywords in Context results into text file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f1bd1-502a-4e78-9bb4-7583bcf93246",
   "metadata": {},
   "source": [
    "### Keyword in Context across multiple files within a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77525fce-0937-4108-9988-8f0e014e99af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591ca78-69f7-464d-898f-ad2174269a2a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set path to your corpus\n",
    "#define that you want to analyze all .txt files in the directory\n",
    "directory_path = 'soderberg-corpus'\n",
    "text_files = glob.glob(f'{directory_path}/*.txt')\n",
    "print(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc92868-4a7f-4e64-a092-c06f28944aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the text files and append tokens to all_docs\n",
    "#This create a list of lists of all tokens from all the documents\n",
    "all_docs = []\n",
    "\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    split_words = re.split(r'\\W+', lowercase_text)\n",
    "    tokenized = [word for word in split_words if word.isalpha()]\n",
    "    return tokenized\n",
    "\n",
    "for filepath in text_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        tokenized_text = tokenize(text)\n",
    "        all_docs.append(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4e904d-ee8c-4973-9c0f-4560c2c27d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords with custom stopwords\n",
    "\n",
    "#Read in custom stopwords txt as list\n",
    "with open(\"custom-stopwords.txt\", \"r\") as file_object:\n",
    "    custom_stopwords = [s.rstrip('\\n') for s in file_object.readlines()] \n",
    "\n",
    "#Define function to remove stopwrods\n",
    "def remove_stopwords(list_of_tokens, stopwords):\n",
    "    return [token for token in list_of_tokens if token not in stopwords]\n",
    "\n",
    "#Loop over all_docs to remove stopwords\n",
    "all_docs_no_stop = []\n",
    "\n",
    "for file in all_docs: \n",
    "    nostop = remove_stopwords(file, custom_stopwords)\n",
    "    all_docs_no_stop.append(nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a3ea98-2e40-4692-ab52-fcbf4cf78f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to return list of ngrams\n",
    "def make_ngrams(tokens, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-(n-1)):\n",
    "        ngrams.append(tokens[i:i+n])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b579e0-9297-4160-9308-671d77956e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to create a dictionary from n-grams, using middle word as the key.\n",
    "#To figure out the keyword for each n-gram we can use the index positions of the list.\n",
    "def ngrams_to_dictionary(ngrams):\n",
    "    keyindex = len(ngrams[0]) // 2\n",
    "\n",
    "    ngram_dictionary = {}\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        if ngram[keyindex] not in ngram_dictionary:\n",
    "            ngram_dictionary[ngram[keyindex]] = [ngram]\n",
    "        else:\n",
    "            ngram_dictionary[ngram[keyindex]].append(ngram)\n",
    "    return ngram_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdfe638-50b9-40b7-94e2-731a44636426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through the files and append the dictionaries for each file\n",
    "#to a list called keywords\n",
    "#Change the number (6) to change the size of the context window \n",
    "#(i.e. the number of words around the keyword)\n",
    "\n",
    "keywords = []\n",
    "\n",
    "for file in all_docs_no_stop:\n",
    "        ngrams = make_ngrams(file, 6)\n",
    "        keywords_in_context = ngrams_to_dictionary(ngrams)\n",
    "        keywords.append(keywords_in_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5999b7bd-f5a9-4630-a892-c883c58722f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that will loop through the list of ngram dictionaries\n",
    "#and print out a given keyword with its ngrams\n",
    "#or print a line that the keyword is not in the dictionary\n",
    "def lookup_keyword(kw, dictionaries):\n",
    "    result_string = \"\"  # Initialize an empty string to collect the results\n",
    "    for i in range(len(dictionaries)):\n",
    "        text_name = text_files[i]\n",
    "        dictionary = dictionaries[i]\n",
    "        if kw in dictionary:\n",
    "            result_string += text_name + \" \" + str(dictionary[kw]) + \"\\n\\n\"\n",
    "        else:\n",
    "            result_string += f\"{kw} not in file: {text_name}\\n\\n\"\n",
    "    return result_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709fe1df-5545-4445-ad36-a73875109dbf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Look up the words that appear next to a given keyword \n",
    "#for each text in the corpus \n",
    "kw_in_context = lookup_keyword('god', keywords)\n",
    "kw_in_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b6e6bd-9a84-426c-b6db-a6254cf3c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out the keywords in context to a file called \"keyword_in_context.txt\"\n",
    "with open(\"keyword_in_context.txt\", mode=\"w\") as file_object:\n",
    "    file_object.write(str(kw_in_context))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54af2693-8d78-485b-ae24-d8ba86240207",
   "metadata": {},
   "source": [
    "#### Most Frequent Neighboring Words across multiple files within a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b46fd32-70e0-48be-ba97-bd26f915802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44aa7d-1b84-47ba-85dd-263c7a902210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look up most frequent words that appear next to a given keyword\n",
    "\n",
    "#Define a function to return most frequent words \n",
    "#that appear next to a particular keyword\n",
    "def get_neighbor_words(keyword, ngrams):\n",
    "    \n",
    "    neighbor_words = []\n",
    "    keyword = keyword.lower()\n",
    "    \n",
    "    for ngram in ngrams:\n",
    "        if keyword in ngram:\n",
    "            for word in ngram:\n",
    "                if word != keyword:\n",
    "                        neighbor_words.append(word)\n",
    "    return Counter(neighbor_words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9296bb7-ab86-403e-9aea-670010832ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through the files and append the ngrams for each file\n",
    "#to a list called all_ngrams\n",
    "#Change the number to change the context window\n",
    "\n",
    "all_ngrams = []\n",
    "\n",
    "for file in all_docs_no_stop:\n",
    "    ngrams = make_ngrams(file, 6)\n",
    "    all_ngrams.append(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e14efa-6b72-48c7-8986-2acc652ad2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to loop through ngrams above\n",
    "# and look up most common neighbor words for a given keyword\n",
    "def lookup_neighbor_words(keyword, ngram_list):\n",
    "    results_string = \"\"  # Initialize an empty string to collect the results\n",
    "    for i in range(len(ngram_list)):\n",
    "        text_name = text_files[i]\n",
    "        text_ngrams = ngram_list[i]\n",
    "        neighbor_words = get_neighbor_words(keyword, text_ngrams)\n",
    "        results_string += text_name + \" \" + str(neighbor_words) + \"\\n\\n\"\n",
    "    return results_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc727d30-e734-4854-a8d6-be617616fac2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Look up the most common neighbor words for a given keyword\n",
    "#for each text in the corpus\n",
    "most_frequent_neighbor_words = lookup_neighbor_words('sun', all_ngrams)\n",
    "most_frequent_neighbor_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67bd803-b194-4078-acfd-bad2ab37080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the most frequent neighbor words to a file called \"most_frequent_neighbor_words.txt\"\n",
    "with open(\"most_frequent_neighbor_words.txt\", mode=\"w\") as file_object:\n",
    "    file_object.write(str(most_frequent_neighbor_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
